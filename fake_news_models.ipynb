{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sys\n",
    "import joblib\n",
    "import spacy\n",
    "#from decouple import config\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,recall_score,precision_score,accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from spacy.lang.en import English\n",
    "##extract punctuation marks from the string\n",
    "punctuations = string.punctuation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "##set your credentials and secret\n",
    "\n",
    "AWS_ID =os.environ.get('AWS_ID')\n",
    "AWS_SECRET_KEY=os.environ.get('AWS_SECRET_KEY')\n",
    "\n",
    "##use the boto3 sdk to integrate python and aws s3\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=AWS_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_KEY)\n",
    "\n",
    "##get the object name and the object key(the actual .csv file)\n",
    "bucket_name = 'edjangobucket'\n",
    "object_key = 'fake_news.csv'\n",
    "\n",
    "csv_object = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "csv_body = csv_object['Body']\n",
    "csv_string = csv_body.read().decode('utf-8')\n",
    "\n",
    "df = pd.read_csv(StringIO(csv_string))\n",
    "##drop the date column\n",
    "df.drop(['date'],axis=1,inplace=True)\n",
    "##print the head and shape of the data\n",
    "print(df.info())\n",
    "##load the english nlp pipeline\n",
    "spacy_eng_token = spacy.load('en_core_web_sm')\n",
    "parser = English()\n",
    "\n",
    "##data cleansing and preprocessing\n",
    "\n",
    "\n",
    "data_process = df[['title','text','subject']]\n",
    "##initially remove urls and special characters symbols from each text\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    ##remove numbers\n",
    "    text_nums=''.join([word for word in text if not word.isdigit()])\n",
    "    ##remove urls\n",
    "    text_urls= re.sub(r'https?://\\S+|www\\.\\S+',\"\", text_nums)\n",
    "    ##remove nicks\n",
    "    text_nicks= re.sub(r\"\\@\\S+\", \"\",text_urls)\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text_nicks.strip().lower()\n",
    "\n",
    "\n",
    "##apply the above helper functions\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    tokens = spacy_eng_token(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    tokens_list = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens ]\n",
    "\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in tokens_list if word not in STOP_WORDS and word not in punctuations ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens\n",
    "\n",
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "\n",
    "\n",
    "##create a vectorizer from the bag of words matrix for our texts\n",
    "bow_vector = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n",
    "print(data_process.head())\n",
    "X=data_process\n",
    "X = np.array(X)\n",
    "\n",
    "Y=df['Type']\n",
    "\n",
    "Y = np.array(Y)\n",
    "\n",
    "##use vectorization feature engineering\n",
    "\n",
    "\n",
    "rnd_clf = RandomForestClassifier(random_state=20)\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "rnd_pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', rnd_clf)])\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "##perform kfold cv\n",
    "print('preparing to split')\n",
    "for train_index,test_index in cv.split(X):\n",
    "    print('split started')\n",
    "    print(train_index,test_index)\n",
    "    X_train,X_test,y_train,y_test = X.iloc[train_index],X.iloc[test_index],y.iloc[train_index],y.iloc[test_index]\n",
    "    rnd_pipe.fit(X_train, y_train)\n",
    "    print(cross_val_score(rnd_pipe, X, Y, cv=5))##got an average score of 0.998\n",
    "    \n",
    "    \n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state=10,shuffle=True,test_size=0.3)\n",
    "        \n",
    "rnd_pipe.fit(X_train,Y_train)\n",
    "\n",
    "#this model gets uploaded to aws s3 and to be reused later\n",
    "joblib.dump(rnd_pipe,'C:\\\\Users\\\\USER\\\\kagglesync\\\\Zindi_Air\\\\fast.joblib')\n",
    "\n",
    "Y_predictions = rnd_pipe.predict(X_test)\n",
    "\n",
    "print(Y_predictions[:20])#[1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 1]\n",
    "\n",
    "\n",
    "##get the metrics for your model\n",
    "print(accuracy_score(Y_test,Y_predictions))# 0.9931403118040089\n",
    "print(classification_report(Y_test,Y_predictions))\n",
    "#            precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      0.99      0.99      5961\n",
    "#            1       0.99      1.00      0.99      5264\n",
    "\n",
    "#     accuracy                           0.99     11225\n",
    "#    macro avg       0.99      0.99      0.99     11225\n",
    "# weighted avg       0.99      0.99      0.99     11225\n",
    "\n",
    "\n",
    "print(precision_score(Y_test,Y_predictions))# 0.9977203647416414\n",
    "print(recall_score(Y_test,Y_predictions))# 0.9877750611246944"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
